<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<!--Converted with LaTeX2HTML 97.1 (release) (July 13th, 1997)
 by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippman, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Lecture 2: GOFAI and Herbert Simon</TITLE>
<META NAME="description" CONTENT="Lecture 2: GOFAI and Herbert Simon">
<META NAME="keywords" CONTENT="NAT">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso_8859_1">
<LINK REL="STYLESHEET" HREF="NAT.css">
<LINK REL="next" HREF="node3.html">
<LINK REL="previous" HREF="node1.html">
<LINK REL="up" HREF="NAT.html">
<LINK REL="next" HREF="node3.html">
</HEAD>
<BODY >
<!--Navigation Panel-->
<A NAME="tex2html31"
 HREF="node3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://www.aber.ac.uk/~dcswww/Images/buttons/next_motif.gif"></A> 
<A NAME="tex2html29"
 HREF="NAT.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://www.aber.ac.uk/~dcswww/Images/buttons/up_motif.gif"></A> 
<A NAME="tex2html23"
 HREF="node1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://www.aber.ac.uk/~dcswww/Images/buttons/previous_motif.gif"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html32"
 HREF="node3.html">Lecture 3: BACON and</A>
<B> Up:</B> <A NAME="tex2html30"
 HREF="NAT.html">No Title</A>
<B> Previous:</B> <A NAME="tex2html24"
 HREF="node1.html">Lecture 1: Introduction</A>
<BR>
<BR>
<!--End of Navigation Panel-->
<H1><A NAME="SECTION00020000000000000000">
Lecture 2: GOFAI and Herbert Simon</A>
</H1>
<P>
<BIG>CS36110, Part 1, Lecture 2:</BIG>
<P><BIG>GOFAI and Herbert Simon</BIG>
<P>
<P>
<BIG>Good Old-Fashioned AI: 1</BIG>
<UL>
<LI> Term (and acronym, GOFAI) invented by John Haugeland (<EM>Artificial
Intelligence: The Very Idea</EM> -- 1983). Haugeland regards it as a
branch of <EM>Cognitive Science</EM>. Otherwise called: ``Symbolic''
(as opposed to ``Subsymbolic'') AI.
<LI> Basic ideas:
<DL COMPACT>
<DT>1.
<DD>Our ability to deal with things intelligently is due to our
ability to think about them reasonably (including subconscious thinking).
<DT>2.
<DD>Our capacity to think about things reasonably amounts to a
faculty for internal ``automatic'' symbol manipulation.
</DL></UL>

<P>
<BIG>Good Old-Fashioned AI: 2</BIG>
<UL>
<LI> GOFAI accounts for <EM>most</EM> of the work done in AI over the past
half-century.
<LI> In the early days (starting with McCulloch and Pitts' ``A
logical calculus of the ideas immanent in nervous activity'' -- 1943)
symbolic and subsymbolic AI were not distinct.
<LI> From (roughly) 1969 to 1986, it accounted for <EM>almost all</EM>
AI research.
<UL>
<LI> 1969: Minsky and Papert's ``Perceptrons: An
Introduction to Computational Geometry''.
<LI> 1986,
McClelland and Rumelhart's ``Parallel Distributed Processing:
Explorations in the Microstructure of Cognition''.
</UL></UL>

<P>
<BIG>Herbert Simon</BIG>
<UL>
<LI> Earliest AI-relevant publication (that I've
found): ``Theory of Automata: Discussion'' <EM>Econometrica</EM> <B>
19</B>, p.72 (1951).
<LI> Most recent: <EM>The Sciences of the Artificial</EM>, 3rd edition
(1996).
<LI> Has also published research in economics (Nobel prize-winner),
decision theory, theory of organizations, psychology, philosophy of
science...
</UL>

<P>
<BIG>Physical Symbol Systems</BIG>
<P>
Simon regards both computers and human brains as <EM>physical symbol
systems</EM>, possessing:
<UL>
<LI> A set of <EM>symbols</EM>: physical patterns that can occur as
components of <EM>expressions</EM>.
<LI> Processes that can <EM>create</EM>, <EM>copy</EM>, <EM>modify</EM> and
<EM>destroy</EM> symbols, thus producing a changing collection of
expressions.
<LI> Ways of acquiring information from outside and encoding
it into internal expressions.
<LI> Ways of affecting the environment guided by
internal expressions (``programs'').
</UL>

<P>
<BIG>The Physical Symbol System Hypothesis</BIG>
<UL>
<LI> ``... the hypothesis is that a physical symbol system... has the
necessary and sufficient means for general intelligent action.''
<LI> ``The hypothesis is clearly an empirical one, to be judged true
or false on the basis of evidence.
<UL>
<LI> ``by constructing computer programs that are demonstrably
capable of intelligent action, we provide evidence on the sufficiency side...''
<LI> ``by collecting experimental data on human thinking that tend to
show that the human brain operates as a symbol system, we add
plausibility to the claims for necessity...''
</UL></UL>

<P>
<BIG>Broad Themes in Simon's Work</BIG>
<UL>
<LI> ``Bounded rationality'' of human decision-making.
<LI> ``Satisficing'' as opposed to ``optimizing'' in problem-solving.
<LI> ``Protocol analysis'' in psychological research.
<LI> Nature of complex systems: hierarchically-structured and
``nearly decomposable''. (Hence, we <EM>don't</EM> need to understand the
brain to understand the mind.
</UL>

<P>
<BIG>Simon's View of the Mind: 1</BIG>
<UL>
<LI> ``Human beings, viewed as behaving systems, are quite
simple. The apparent complexity of our behavior over time is largely a
reflection of the complexity of the environment.'' (Sciences of the
Artificial, p.53).
<LI> Basis for this view: nature of humans as <EM>adaptive</EM>
systems. Small number of intrinsic characteristics of the mind limit
its adaptivity (e.g. how much can be held in short-term memory).
<LI> The long-term memory is viewed ``less as a part of the organism
than as part of the environment to which it adapts''.
</UL>

<P>
<BIG>Simon's View of the Mind: 2</BIG>
<UL>
<LI> Long-term memory can be seen as <EM>artificial</EM> aid to
problem-solving (like books, computers, etc.)
<LI> Thought is fundamentally <EM>serial</EM>.
<LI> No fundamental difference between conscious and unconscious
problem-solving.
<LI> Human learning is slow and inefficient.
<LI> Emotion can be viewed as a <EM>prioritizing</EM> mechanism,
enabling one ``program'' to ``interrupt'' another.
</UL>

<P>
<BIG>Aspects of Simon's AI Research</BIG>
<UL>
<LI> Early concentration on ``formal'' tasks and ``microworlds''
(formal logic, chess, cryptarithmetic puzzles).
``If one could devise a successful chess machine, one would seem to
have penetrated to the core of human intellectual endeavor.''
<LI> Continued concentration on modelling human behavior as revealed
by psychological experiments and protocol analysis. EPAM (rote verbal
learning), GPS (``general'' problem solving), BACON (scientific
discovery).
<LI> Importance of <EM>selective, heuristic</EM> search processes.
</UL>

<P>
<BIG>Overview of GPS</BIG>
<UL>
<LI> First program (1959) to make a clear separation between <EM>
domain-specific knowledge</EM> and
<BR>
<EM>problem-solving strategy</EM>.
<LI> Can be regarded as implementing <EM>heuristic search</EM>, guided
by <EM>means-end analysis</EM>.
<LI> GPS `maximally confuses' the attempt to <EM>accomplish</EM> tasks humans
perform, and the attempt to <EM>simulate</EM> human performance.
<LI> Used to model a human novice's approach to problems of
manipulating logical formulae.
</UL>

<P>
<BIG>Some Significant Features of GPS</BIG>
<UL>
<LI> Written in low-level list-processing language, IPL.
<LI> Made use of three types of goals:
<UL>
<LI> Transform object A into object B.
<LI> Reduce difference D between object A and object B.
<LI> Apply operator Q to object A.
</UL>
<LI> Goals achieved by breaking them down into a tree of
subgoals, using methods illustrated on the next three slides.
</UL>

<P>
<BIG>Goal: Transform Object A</BIG>
<BR>
<BIG>Into Object B</BIG>
<P>
<DIV ALIGN="CENTER">
<IMG WIDTH="334" HEIGHT="540" ALIGN="BOTTOM" BORDER="0"
 SRC="img2.gif"
 ALT="\epsfig {figure=sl3.1.eps}
"></DIV>

<P>
<BIG>Goal: Reduce Difference D</BIG>
<BR>
<BIG>Between Object A and Object B</BIG>
<P>
<DIV ALIGN="CENTER">
<IMG WIDTH="300" HEIGHT="540" ALIGN="BOTTOM" BORDER="0"
 SRC="img3.gif"
 ALT="\epsfig {figure=sl3.2.eps}
"></DIV>

<P><BIG>Goal: Apply Operator Q</BIG>
<BR>
<BIG>To Object A</BIG>
<P>
<DIV ALIGN="CENTER">
<IMG WIDTH="392" HEIGHT="540" ALIGN="BOTTOM" BORDER="0"
 SRC="img4.gif"
 ALT="\epsfig {figure=sl3.3.eps}
"></DIV>

<P>
<BIG>Some Limitations of GPS</BIG>
<UL>
<LI> ``Lack of Intermediate Information'' problem: if unsuccessful,
no information about the cause of failure is returned.
<LI> ``Lack of Descriptive Power'' problem: there are many aspects of
the world that cannot adequately be expressed in terms of applying
operators to objects: spatial and temporal relations, continuous
processes of change in the world, probabilities, beliefs and
preferences.
<LI> ``Interacting Goals'' problem: people have complex, interacting,
often ill-defined goals.
<LI> ``Perfect Information'' problem: people do not know the full
state of the world (and are aware of this). 
</UL>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html31"
 HREF="node3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://www.aber.ac.uk/~dcswww/Images/buttons/next_motif.gif"></A> 
<A NAME="tex2html29"
 HREF="NAT.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://www.aber.ac.uk/~dcswww/Images/buttons/up_motif.gif"></A> 
<A NAME="tex2html23"
 HREF="node1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://www.aber.ac.uk/~dcswww/Images/buttons/previous_motif.gif"></A>   
<BR>
<B> Next:</B> <A NAME="tex2html32"
 HREF="node3.html">Lecture 3: BACON and</A>
<B> Up:</B> <A NAME="tex2html30"
 HREF="NAT.html">No Title</A>
<B> Previous:</B> <A NAME="tex2html24"
 HREF="node1.html">Lecture 1: Introduction</A>
<!--End of Navigation Panel-->
<ADDRESS>
<I>NICHOLAS MARK GOTTS</I>
<BR><I>5/4/1998</I>
</ADDRESS>
</BODY>
</HTML>
